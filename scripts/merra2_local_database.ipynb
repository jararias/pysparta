{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import blosc2\n",
    "from scipy.interpolate import interp1d\n",
    "from loguru import logger\n",
    "\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import pylab as pl\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "logger.enable(__name__)\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level='INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a92ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncFilter:\n",
    "    def __init__(self, precision, dtype='f4', astype='i4'):\n",
    "        self._fill_value = -999\n",
    "        self._data_type = dtype\n",
    "        self._store_type = astype\n",
    "        self._precision = int(precision)\n",
    "        self._scale_factor = 10**self._precision\n",
    "\n",
    "    def encode(self, data):\n",
    "        values = self._scale_factor * np.round(data.astype(self._data_type), self._precision)\n",
    "        return np.where(np.isnan(values), self._fill_value, values).astype(self._store_type)\n",
    "\n",
    "    def decode(self, data):\n",
    "        return (np.where(data == self._fill_value, np.nan, data) / self._scale_factor).astype(self._data_type)\n",
    "\n",
    "\n",
    "def save_array(path, variable_name, values,\n",
    "               precision, dtype='f4', astype='i4', compressor=None,\n",
    "               fill_value=-999, test_accuracy=True):\n",
    "\n",
    "    if not (p := Path(path)).exists():\n",
    "        logger.debug(f'Creating data path `{p}`')\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    trunc = TruncFilter(precision, dtype, astype)\n",
    "    cparams = {'codec': 'ZSTD', 'clevel': 9}\n",
    "    cparams.update(compressor or {})\n",
    "    if isinstance(cparams['codec'], str):\n",
    "        cparams['codec'] = blosc2.Codec[cparams['codec']]\n",
    "\n",
    "    kwargs = dict(\n",
    "        cparams=cparams\n",
    "    )\n",
    "\n",
    "    file_name = p / f'{variable_name}.bl2'\n",
    "    if file_name.exists():\n",
    "        file_name.unlink()\n",
    "        time.sleep(1)  # otherwise, the save_array function sometimes fails!\n",
    "    file_size = blosc2.save_array(trunc.encode(values), file_name.as_posix(), **kwargs)\n",
    "    logger.debug(f'data array saved in `{file_name}` ({file_size / 1024: .2f} Kb)')\n",
    "\n",
    "    cparams['codec'] = cparams['codec'].name\n",
    "    update_metadata(\n",
    "        path, variable_name,\n",
    "        compressor=cparams,\n",
    "        filters=[\n",
    "            {'class': 'TruncFilter',\n",
    "             'kwargs': {'precision': precision, 'dtype': dtype, 'astype': astype}}\n",
    "        ])\n",
    "\n",
    "    if test_accuracy is True:\n",
    "        xvalues = load_array(path, variable_name)\n",
    "        residue = values - xvalues\n",
    "        mbe, rmse = np.nanmean(residue), np.nanmean(residue**2)**0.5\n",
    "        if abs(mobs := np.nanmean(values)) > 1e-4:\n",
    "            mbe = f'{mbe/mobs:.1%}'\n",
    "            rmse = f'{rmse/mobs:.1%}'\n",
    "        else:\n",
    "            mbe = f'{mbe}'\n",
    "            rmse = f'{rmse}'\n",
    "        mad = f'{np.nanmax(np.abs(residue)):10.3e}'\n",
    "        logger.info(f'Compression accuracy of {variable_name}:  {mbe=}  {rmse=}  Max. Abs. Diff.={mad}')\n",
    "\n",
    "    return file_size\n",
    "\n",
    "\n",
    "def load_array(path, variable):\n",
    "    \"\"\"do not remove. It is used by save_array to make consistency checks\"\"\"\n",
    "    file_name = Path(path) / f'{variable}.bl2'\n",
    "    metadata = json.load(open(Path(path) / 'metadata.json', 'r'))\n",
    "    data = blosc2.load_array(file_name.as_posix())\n",
    "\n",
    "    visible_modules = sys.modules[__name__]\n",
    "    visible_class_names, visible_classes = zip(*inspect.getmembers(visible_modules, inspect.isclass))\n",
    "    for filter_descr in metadata[variable]['filters']:\n",
    "        filter_cls_name = filter_descr['class']\n",
    "        filter_cls_kwargs = filter_descr['kwargs']\n",
    "        if filter_cls_name not in visible_class_names:\n",
    "            raise ValueError(f'unknown filter of type `{cfilter[\"class\"]}`')\n",
    "        filter_cls = visible_classes[visible_class_names.index(filter_cls_name)]\n",
    "        cfilter = filter_cls(**filter_cls_kwargs)\n",
    "        data = cfilter.decode(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def update_metadata(path, variable=None, latitude=None, longitude=None, **options):\n",
    "\n",
    "    file_name = Path(path) / 'metadata.json'\n",
    "\n",
    "    if not file_name.exists():\n",
    "        logger.debug(f'Creating metadata file `{file_name}`')\n",
    "        json.dump({'Created': time.ctime()}, file_name.open(mode='w', encoding='utf-8'))\n",
    "\n",
    "    metadata = json.load(file_name.open(mode='r', encoding='utf-8'))\n",
    "\n",
    "    entry = metadata\n",
    "    if variable is not None:\n",
    "        if variable not in metadata:\n",
    "            metadata[variable] = {'Created': time.ctime()}\n",
    "        entry = metadata[variable]\n",
    "        entry['Last updated'] = time.ctime()\n",
    "\n",
    "    if latitude is not None:\n",
    "        logger.debug(f'adding entry `latitude` to metadata/{variable or \"_root_\"}')\n",
    "        entry['latitude'] = {\n",
    "            'start': latitude[0].item(),\n",
    "            'end': latitude[-1].item(),\n",
    "            'step': np.unique(np.diff(latitude)).item()\n",
    "        }\n",
    "\n",
    "    if longitude is not None:\n",
    "        logger.debug(f'adding entry `longitude` to metadata/{variable or \"_root_\"}')\n",
    "        entry['longitude'] = {\n",
    "            'start': longitude[0].item(),\n",
    "            'end': longitude[-1].item(),\n",
    "            'step': np.unique(np.diff(longitude)).item()\n",
    "        }\n",
    "\n",
    "    # if there are other options provided...\n",
    "    for key, value in options.items():\n",
    "        logger.debug(f'adding entry `{key}` to metadata/{variable or \"_root_\"}')\n",
    "        entry[key] = value\n",
    "\n",
    "    json.dump(metadata, file_name.open(mode='w', encoding='utf-8'))\n",
    "\n",
    "\n",
    "def read_netcdf(file_name, variable):\n",
    "    with netCDF4.Dataset(file_name, 'r') as cdf:\n",
    "        values = np.array(cdf.variables[variable][:], dtype=np.float32)\n",
    "        lon = np.array(cdf.variables['lon'][:], dtype=np.float32)\n",
    "        lat = np.array(cdf.variables['lat'][:], dtype=np.float32)\n",
    "        try:\n",
    "            time = cdf.variables['time']\n",
    "            times = netCDF4.num2date(time[:], units=time.units)\n",
    "        except KeyError:\n",
    "            times = None\n",
    "    return values, lon, lat, times\n",
    "\n",
    "\n",
    "def regrid(grid_x, grid_y, grid_z, x, y, method='bilinear'):\n",
    "    \"\"\"Interpolation along axes (-2,-1) in rank-n grid_z.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_x: array-like, rank 1 of shape (N,)\n",
    "        coordinate values along dimension x of grid_z (axis=-1)\n",
    "    grid_y: array-like, rank 1 of shape (M,)\n",
    "        coordinate values along dimension y of grid_z (axis=-2)\n",
    "    grid_z: array-like, rank n of shape (..., M, N)\n",
    "    x: array-like, arbitrary shape, typically rank-1 or rank-2 array\n",
    "        target coordinate values for dimension x. Must have same shape as y\n",
    "    y: array-like, arbitrary shape, typically rank-1 or rank-2 array\n",
    "        target coordinate values for dimension y. Must have same shape as x\n",
    "    method: str\n",
    "        interpolation method: nearest or bilinear\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Interpolated values in an array with shape (..., shape of x and y). For\n",
    "    instance, if grid_z has dimensions (dfb, slot, latitude, longitude) and x\n",
    "    and y are rank-2 longitude and latitude arrays, respectively, with shape\n",
    "    (P, Q), the output array would have shape (dfb, slot, P, Q). In contrast,\n",
    "    if the new locations were rank-1 arrays with shape (R,), the shape\n",
    "    of the output array would be (dfb, slot, R). Same comments apply to input\n",
    "    arrays with shapes (time, latitude, longitude) or (issue_day, cycle,\n",
    "    lead_hour, latitude, longitude), for instance.\n",
    "    \"\"\"\n",
    "    # transformation to the segment (0,1)x(0,1)\n",
    "    def normalize(v, grid):\n",
    "        return (v - grid[0]) / (grid[-1] - grid[0])\n",
    "    ycoords = normalize(grid_y, grid_y)\n",
    "    xcoords = normalize(grid_x, grid_x)\n",
    "    yinterp = normalize(y, grid_y)\n",
    "    xinterp = normalize(x, grid_x)\n",
    "\n",
    "    zvalues = grid_z\n",
    "    if np.ma.is_masked(zvalues):\n",
    "        zvalues = np.where(zvalues.mask, np.nan, zvalues.data)\n",
    "    assert zvalues.ndim >= 2, \\\n",
    "        'grid_val must have at least ndim=2. Got {}'.format(zvalues.ndim)\n",
    "\n",
    "    def clip(k, kmax):\n",
    "        return np.clip(k, 0, kmax)\n",
    "\n",
    "    if method == 'nearest':\n",
    "        jx = np.rint((grid_y.size - 1) * yinterp).astype('int')\n",
    "        ix = np.rint((grid_x.size - 1) * xinterp).astype('int')\n",
    "        jx = clip(jx, grid_y.size - 1)\n",
    "        ix = clip(ix, grid_x.size - 1)\n",
    "        return zvalues[..., jx, ix]\n",
    "\n",
    "    elif method == 'bilinear':\n",
    "        j1 = ((grid_y.size - 1) * yinterp).astype('int')\n",
    "        i1 = ((grid_x.size - 1) * xinterp).astype('int')\n",
    "        jmax, imax = grid_y.size - 1, grid_x.size - 1\n",
    "        Axy = (ycoords[clip(j1 + 1, jmax)] - ycoords[clip(j1, jmax)]) * \\\n",
    "            (xcoords[clip(i1 + 1, imax)] - xcoords[clip(i1, imax)])\n",
    "        A11 = (ycoords[clip(j1 + 1, jmax)] - yinterp) * \\\n",
    "            (xcoords[clip(i1 + 1, imax)] - xinterp) / Axy\n",
    "        A12 = (ycoords[clip(j1 + 1, jmax)] - yinterp) * \\\n",
    "            (xinterp - xcoords[clip(i1, imax)]) / Axy\n",
    "        A21 = (yinterp - ycoords[clip(j1, jmax)]) * \\\n",
    "            (xcoords[clip(i1 + 1, imax)] - xinterp) / Axy\n",
    "        A22 = (yinterp - ycoords[clip(j1, jmax)]) * \\\n",
    "            (xinterp - xcoords[clip(i1, imax)]) / Axy\n",
    "        return (zvalues[..., clip(j1, jmax), clip(i1, imax)] * A11 +\n",
    "                zvalues[..., clip(j1, jmax), clip(i1 + 1, imax)] * A12 +\n",
    "                zvalues[..., clip(j1 + 1, jmax), clip(i1, imax)] * A21 +\n",
    "                zvalues[..., clip(j1 + 1, jmax), clip(i1 + 1, imax)] * A22)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('unknown interpolation method %r' % method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58a0d4",
   "metadata": {},
   "source": [
    "## LTA DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = [('albedo', 3), ('pressure', 0), ('ozone', 3), ('pwater', 2),\n",
    "             ('alpha', 2), ('beta', 3), ('ssa', 3), ('elevation', 0)]\n",
    "\n",
    "def get_target_path(**kwargs):\n",
    "    return 'lta'.format(**kwargs)\n",
    "\n",
    "def get_source_file_name(**kwargs):\n",
    "    db_root = Path('/home/jararias/.solarpandas-data/merra2_lta/2010-2021')\n",
    "    if kwargs['variable'] == 'elevation':\n",
    "        return db_root / 'merra2_elevation.nc4'\n",
    "    return db_root / 'merra2_{variable}_lta_2010-2021.nc4'.format(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633889a",
   "metadata": {},
   "source": [
    "### Create dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723396fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable, precision in VARIABLES:\n",
    "    logger.info(f'Processing variable `{variable}`')\n",
    "\n",
    "    file_name = get_source_file_name(variable=variable)\n",
    "    if not file_name.exists():\n",
    "        logger.warning(f'missing file `{file_name}`. Skipping')\n",
    "\n",
    "    path = get_target_path()\n",
    "\n",
    "    values, lon, lat, _ = read_netcdf(file_name, variable)\n",
    "    values = values[0] if variable == 'elevation' else values\n",
    "\n",
    "    save_array(path, variable, values=values, precision=precision)\n",
    "    update_metadata(path, variable, latitude=lat, longitude=lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b298f7",
   "metadata": {},
   "source": [
    "### Dataset reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTADataset:\n",
    "    def __init__(self, path):\n",
    "        self._path = Path(path)\n",
    "        metadata_file_name = self._path / 'metadata.json'\n",
    "        if not metadata_file_name.exists():\n",
    "            raise ValueError(f'missing required file `{metadata_file_name}`')\n",
    "        self._metadata = json.load(metadata_file_name.open())\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return [key for key, value in self._metadata.items() if key != 'elevation' and isinstance(value, dict)]\n",
    "\n",
    "    def has_variable(self, variable):\n",
    "        return variable in self.variables\n",
    "\n",
    "    def get_latitude(self, variable):\n",
    "        if (not self.has_variable(variable)) and (variable != 'elevation'):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "        kwargs = self._metadata[variable]['latitude']\n",
    "        return np.arange(kwargs['start'], kwargs['end']+1e-6, kwargs['step'])\n",
    "\n",
    "    def get_longitude(self, variable):\n",
    "        if (not self.has_variable(variable)) and (variable != 'elevation'):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "        kwargs = self._metadata[variable]['longitude']\n",
    "        return np.arange(kwargs['start'], kwargs['end']+1e-6, kwargs['step'])\n",
    "\n",
    "    def get_elevation(self):\n",
    "        return self._load_array('elevation')\n",
    "\n",
    "    def get(self, variable, times=None, lons=None, lats=None, regrid_method='bilinear'):\n",
    "        if not self.has_variable(variable):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "\n",
    "        data_lats = self.get_latitude(variable)\n",
    "        data_lons = self.get_longitude(variable)\n",
    "        data = self._load_array(variable)\n",
    "\n",
    "        if (lons is None and lats is not None) or (lons is not None and lats is None):\n",
    "            raise ValueError('lats and lons, or none of them, must be provided')\n",
    "\n",
    "        if lons is not None and lats is not None:\n",
    "            # spatial regridding...\n",
    "            target_lons = np.array(lons, ndmin=1)\n",
    "            target_lats = np.array(lats, ndmin=1)\n",
    "            data = regrid(\n",
    "                data_lons, data_lats, data,\n",
    "                target_lons, target_lats, method=regrid_method)\n",
    "\n",
    "            data_lats = target_lats\n",
    "            data_lons = target_lons\n",
    "\n",
    "        if times is not None:\n",
    "            # expand dataset in the temporal dimension to span the target period...\n",
    "            target_times = np.array(times, dtype='datetime64[ns]')\n",
    "            years = np.unique(target_times.astype('datetime64[Y]')).astype('i2') + 1970\n",
    "            expanded_times = np.array(\n",
    "                [f'{yr}-{mo:02d}-15' for yr in years for mo in range(1, 13)],\n",
    "                dtype='datetime64[ns]')\n",
    "            expanded_data = np.vstack([data for _ in range(len(years))])\n",
    "\n",
    "            # perform the temporal interpolation to the target times...\n",
    "            kwargs = dict(kind=2, fill_value='extrapolate')\n",
    "            xi = self._get_fractional_year(expanded_times)\n",
    "            x = self._get_fractional_year(target_times)\n",
    "            data = interp1d(xi, expanded_data, axis=0, **kwargs)(x)\n",
    "\n",
    "        if np.isscalar(lons) and np.isscalar(lats):\n",
    "            return data[:, 0]\n",
    "        return data\n",
    "\n",
    "    def get_atmos(self, **kwargs):\n",
    "        with cf.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "\n",
    "            futures = {executor.submit(self.get, variable, **kwargs): variable\n",
    "                       for variable in self.variables}\n",
    "            logger.debug('futures submitted!!')\n",
    "\n",
    "            data = {}\n",
    "            for future in cf.as_completed(futures):\n",
    "                variable = futures[future]\n",
    "                logger.debug(f'variable `{variable}` completed')\n",
    "                try:\n",
    "                    data[variable] = future.result()\n",
    "                except Exception as exc:\n",
    "                    logger.error(f'the thread for variable `{variable}` generated and exception: {exc}')\n",
    "\n",
    "            return data\n",
    "\n",
    "    def _get_fractional_year(self, times):\n",
    "        one_day = np.timedelta64(1, 'D')\n",
    "        one_year = np.timedelta64(1, 'Y')\n",
    "        jan_1st = times.astype('datetime64[Y]').astype('datetime64[D]')\n",
    "        dec_31st = jan_1st.astype('datetime64[Y]') + one_year - one_day\n",
    "        year_length = dec_31st - jan_1st + one_day\n",
    "        year_fraction = (times - jan_1st) / year_length\n",
    "        return times.astype('datetime64[Y]').astype('f4') + 1970 + year_fraction\n",
    "\n",
    "    def _load_array(self, variable):\n",
    "        if (not self.has_variable(variable)) and (variable != 'elevation'):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "            \n",
    "        file_name = self._path / f'{variable}.bl2'\n",
    "        data = blosc2.load_array(file_name.as_posix())\n",
    "\n",
    "        visible_modules = sys.modules[__name__]\n",
    "        visible_class_names, visible_classes = zip(*inspect.getmembers(visible_modules, inspect.isclass))\n",
    "        for filter_descr in self._metadata[variable]['filters']:\n",
    "            filter_cls_name = filter_descr['class']\n",
    "            filter_cls_kwargs = filter_descr['kwargs']\n",
    "            if filter_cls_name not in visible_class_names:\n",
    "                raise ValueError(f'unknown filter of type `{cfilter[\"class\"]}`')\n",
    "            filter_cls = visible_classes[visible_class_names.index(filter_cls_name)]\n",
    "            cfilter = filter_cls(**filter_cls_kwargs)\n",
    "            data = cfilter.decode(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59508a",
   "metadata": {},
   "source": [
    "### Explore data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ed6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merra2_lta = LTADataset('lta')\n",
    "merra2_lta.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0888a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(12, 6))\n",
    "ax = pl.subplot(111, projection=ccrs.PlateCarree())\n",
    "lat = merra2_lta.get_latitude('elevation')\n",
    "lon = merra2_lta.get_longitude('elevation')\n",
    "elev = merra2_lta.get_elevation()\n",
    "pc = ax.pcolormesh(lon, lat, elev, cmap='terrain', vmin=-1400)\n",
    "ax.coastlines(linewidth=0.5, color='w')\n",
    "pl.colorbar(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atmos = merra2_lta.get_atmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e53b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pl.subplots(len(atmos), 12, figsize=(12, 0.5*len(atmos)))\n",
    "for k_variable, variable in enumerate(atmos.keys()):\n",
    "    for k_month, month in enumerate(range(1, 13)):\n",
    "        ax = axes[k_variable, k_month]\n",
    "        ax.pcolormesh(atmos[variable][k_month], cmap='jet')\n",
    "        ax.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
    "        if k_variable == 0:\n",
    "            ax.set_title(f'month={month}', fontsize=6)\n",
    "        if k_month == 0:\n",
    "            ax.set_ylabel(variable, fontsize=6)\n",
    "logger.info('plotting...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'pwater'\n",
    "target_times = pd.date_range('2019-12-01T20', '2020-12-14T04', freq='D')\n",
    "target_lons, target_lats = -3.5, 37.5\n",
    "target_lons, target_lats = np.arange(-10, 10, 0.5), np.arange(35, 55, 0.5)\n",
    "target_lons, target_lats = np.meshgrid(np.arange(-10, 10, 0.5), np.arange(35, 55, 0.5))\n",
    "\n",
    "interp_data = merra2_lta.get(variable, times=target_times, lons=target_lons, lats=target_lats)\n",
    "\n",
    "pl.plot(target_times, regrid(target_lons[0], target_lats[:, 0], interp_data, -3.5, 37.5))\n",
    "\n",
    "# expand dataset in the temporal dimension to span the target period...\n",
    "years = np.unique(np.array(target_times, dtype='datetime64[ns]').astype('datetime64[Y]')).astype('i2') + 1970\n",
    "expanded_times = np.array(\n",
    "    [f'{yr}-{mo:02d}-15' for yr in years for mo in range(1, 13)],\n",
    "    dtype='datetime64[ns]')\n",
    "source_lats = merra2_lta.get_latitude(variable)\n",
    "source_lons = merra2_lta.get_longitude(variable)\n",
    "source_data = merra2_lta.get(variable)\n",
    "regridded_data = regrid(source_lons, source_lats, source_data,\n",
    "                        np.array(target_lons, ndmin=1), np.array(target_lats, ndmin=1))\n",
    "expanded_data = np.vstack([regridded_data for _ in range(len(years))])\n",
    "pl.plot(expanded_times, regrid(target_lons[0], target_lats[:, 0], expanded_data, -3.5, 37.5), ls='', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9381f",
   "metadata": {},
   "source": [
    "## DAILY DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = [('albedo', 3), ('pressure', 0), ('ozone', 3), ('pwater', 2),\n",
    "             ('alpha', 2), ('beta', 3), ('ssa', 3), ('elevation', 0)]\n",
    "\n",
    "def get_target_path(**kwargs):\n",
    "    return 'daily/{year}'.format(**kwargs)\n",
    "\n",
    "def get_source_file_name(**kwargs):\n",
    "    db_root = Path('/home/jararias/.solarpandas-data/merra2_daily')\n",
    "    if kwargs['variable'] == 'elevation':\n",
    "        return db_root / 'merra2_elevation.nc4'\n",
    "    return db_root / '{variable}/merra2_{variable}_daily_time_chunked_{year}.nc4'.format(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851b2df",
   "metadata": {},
   "source": [
    "### Create dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1999\n",
    "for variable, precision in VARIABLES:\n",
    "    logger.info(f'Processing variable `{variable}` / year {year}')\n",
    "\n",
    "    path = get_target_path(year=year)\n",
    "\n",
    "    file_name = get_source_file_name(variable=variable, year=year)\n",
    "    if not file_name.exists():\n",
    "        logger.warning(f'missing file `{file_name}`. Skipping')\n",
    "\n",
    "    values, lon, lat, times = read_netcdf(file_name, variable)\n",
    "\n",
    "    if variable == 'elevation':\n",
    "        save_array(path, variable, values=values[0], precision=precision)\n",
    "        update_metadata(path, variable, latitude=lat, longitude=lon)\n",
    "        continue\n",
    "\n",
    "    time_start = np.datetime64(times[0], 'ns')\n",
    "    time_end = np.datetime64(times[-1], 'ns')\n",
    "\n",
    "    # CREATE A 3-DAYS TIME HALO (where possible)...\n",
    "    #   this halo guarantees that times can be extracted from the very beginning\n",
    "    #   and very end of the year without using extrapolation while keeping the\n",
    "    #   time series continuity in the transition from one year to the following\n",
    "\n",
    "    # previous file...\n",
    "    file_name = get_source_file_name(variable=variable, year=year-1)\n",
    "    if file_name.exists():\n",
    "        prev_values, _, _, times = read_netcdf(file_name, variable)\n",
    "        values = np.r_[prev_values[-3:, ...], values]\n",
    "        time_start = np.datetime64(times[-3], 'ns')\n",
    "\n",
    "    # next file...\n",
    "    file_name = get_source_file_name(variable=variable, year=year+1)\n",
    "    if file_name.exists():\n",
    "        next_values, _, _, times = read_netcdf(file_name, variable)\n",
    "        values = np.r_[values, next_values[:3, ...]]\n",
    "        time_end = np.datetime64(times[2], 'ns')\n",
    "\n",
    "    save_array(path, variable, values=values, precision=precision)\n",
    "    times={'start': str(time_start), 'end': str(time_end), 'delta': [1, 'D']}\n",
    "    update_metadata(path, variable, latitude=lat, longitude=lon, times=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ffe65",
   "metadata": {},
   "source": [
    "### Dataset reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d84c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyDataset:\n",
    "    def __init__(self, path):\n",
    "        self._path = Path(path)\n",
    "        metadata_file_name = self._path / 'metadata.json'\n",
    "        if not metadata_file_name.exists():\n",
    "            raise ValueError(f'missing required file `{metadata_file_name}`')\n",
    "        self._metadata = json.load(metadata_file_name.open())\n",
    "\n",
    "        self._variables = [key for key, value in self._metadata.items()\n",
    "                           if key != 'elevation' and isinstance(value, dict)]\n",
    "\n",
    "        year = []\n",
    "        one_ns = np.timedelta64(1, 'ns')\n",
    "        for variable in self._variables:\n",
    "            start = self._metadata[variable]['times']['start']\n",
    "            stop = self._metadata[variable]['times']['end']\n",
    "            step = np.timedelta64(*self._metadata[variable]['times']['delta'])\n",
    "            times = np.arange(start, np.datetime64(stop) + one_ns, step)\n",
    "            # detect the year that corresponds to this dataset...\n",
    "            years, counts = np.unique(times.astype('datetime64[Y]'), return_counts=True)\n",
    "            year.append(years[np.argmax(counts)])\n",
    "\n",
    "        try:\n",
    "            self._year = np.unique(year).item().year\n",
    "        except ValueError:\n",
    "            raise ValueError(f'dataset spanning multiple years: {np.unique(year)}')\n",
    "\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._variables\n",
    "\n",
    "    @property\n",
    "    def year(self):\n",
    "        return self._year\n",
    "\n",
    "    def has_variable(self, variable):\n",
    "        return variable in self.variables\n",
    "\n",
    "    def get_latitude(self, variable):\n",
    "        if (not self.has_variable(variable)) and (variable != 'elevation'):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "        kwargs = self._metadata[variable]['latitude']\n",
    "        return np.arange(kwargs['start'], kwargs['end']+1e-6, kwargs['step'])\n",
    "\n",
    "    def get_longitude(self, variable):\n",
    "        if not self.has_variable(variable) and (variable != 'elevation'):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "        kwargs = self._metadata[variable]['longitude']\n",
    "        return np.arange(kwargs['start'], kwargs['end']+1e-6, kwargs['step'])\n",
    "\n",
    "    def get_times(self, variable):\n",
    "        if not self.has_variable(variable):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "        one_ns = np.timedelta64(1, 'ns')\n",
    "        start = self._metadata[variable]['times']['start']\n",
    "        stop = self._metadata[variable]['times']['end']\n",
    "        step = np.timedelta64(*self._metadata[variable]['times']['delta'])\n",
    "        return np.arange(start, np.datetime64(stop) + one_ns, step)\n",
    "\n",
    "    def get_elevation(self):\n",
    "        return self._load_array('elevation')\n",
    "\n",
    "    def get(self, variable, times=None, lons=None, lats=None, regrid_kwargs=None, interp_kwargs=None):\n",
    "        if not self.has_variable(variable):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "\n",
    "        logger.debug(f'reading data on variable `{variable}`')\n",
    "        data_times = self.get_times(variable)\n",
    "        data_lons = self.get_longitude(variable)\n",
    "        data_lats = self.get_latitude(variable)\n",
    "        data = self._load_array(variable)\n",
    "\n",
    "        # ...and set the time limits for the dataset (even if it has a halo)\n",
    "        data_time_min = np.datetime64(self.year-1970, 'Y').astype('datetime64[ns]')\n",
    "        data_time_max = np.datetime64(self.year-1970 + 1, 'Y').astype('datetime64[ns]')\n",
    "        logger.debug(f'dataset year: {self.year}  min. time: '\n",
    "                     f'{data_time_min}  max. time: {data_time_max}')\n",
    "\n",
    "        if (lons is None and lats is not None) or (lons is not None and lats is None):\n",
    "            raise ValueError('lats and lons, or none of them, must be provided')\n",
    "\n",
    "        if lons is not None and lats is not None:\n",
    "            logger.debug(f'regridding on variable `{variable}`')\n",
    "            # spatial regridding...\n",
    "            kwargs = {'method': 'bilinear'}\n",
    "            kwargs.update(regrid_kwargs or {})\n",
    "            target_lons = np.array(lons, ndmin=1)\n",
    "            target_lats = np.array(lats, ndmin=1)\n",
    "            data = regrid(data_lons, data_lats, data, target_lons, target_lats, **kwargs)\n",
    "\n",
    "            data_lats = target_lats\n",
    "            data_lons = target_lons\n",
    "\n",
    "        if times is not None:\n",
    "            # perform the temporal interpolation to the target times...\n",
    "            logger.debug(f'temporal interpolation on variable `{variable}`')\n",
    "            # the variables that changes smoothly throughout time are interpolated\n",
    "            # linearly to save time. If there are nan values in any array, use linear\n",
    "            # interpolation because quadratic and cubic then fill all with nans\n",
    "            kind = {'beta': 2, 'alpha': 2, 'pwater': 2}.get(variable, 1)\n",
    "            if np.any(np.isnan(data)):\n",
    "                kind = 1\n",
    "            kwargs = {'kind': kind, 'fill_value': np.nan}\n",
    "            kwargs.update(interp_kwargs or {})\n",
    "            if interp_kwargs:\n",
    "                logger.debug(f'{kwargs=}')\n",
    "\n",
    "            target_times = np.array(times, dtype='datetime64[ns]')\n",
    "\n",
    "            xi = data_times.astype('f8')\n",
    "            x = target_times.astype('f8')\n",
    "            data = interp1d(xi, data, axis=0, **kwargs)(x)\n",
    "\n",
    "            mask = (target_times < data_time_min) | (target_times >= data_time_max)\n",
    "            data[mask] = np.nan\n",
    "            data_times = target_times\n",
    "\n",
    "        if np.isscalar(lons) and np.isscalar(lats):\n",
    "            logger.debug(f'\"{variable}\".shape={data[:, 0].shape}')\n",
    "            return data[:, 0]\n",
    "\n",
    "        logger.debug(f'\"{variable}\".shape={data.shape}')\n",
    "        return data\n",
    "\n",
    "    def get_atmos(self, **kwargs):\n",
    "        import concurrent.futures as cf\n",
    "        with cf.ThreadPoolExecutor(max_workers=len(self.variables)) as executor:\n",
    "\n",
    "            futures = {executor.submit(self.get, variable, **kwargs): variable\n",
    "                       for variable in self.variables}\n",
    "            logger.debug('futures submitted!!')\n",
    "\n",
    "            data = {}\n",
    "            for future in cf.as_completed(futures):\n",
    "                variable = futures[future]\n",
    "                logger.debug(f'variable `{variable}` completed')\n",
    "                try:\n",
    "                    data[variable] = future.result()\n",
    "                except Exception as exc:\n",
    "                    logger.error(f'the thread for variable `{variable}` generated and exception: {exc}')\n",
    "\n",
    "            return data\n",
    "\n",
    "    def _load_array(self, variable):\n",
    "        if (not self.has_variable(variable)) and (variable != 'elevation'):\n",
    "            raise ValueError(f'missing variable `{variable}`')\n",
    "            \n",
    "        file_name = self._path / f'{variable}.bl2'\n",
    "        data = blosc2.load_array(file_name.as_posix())\n",
    "\n",
    "        visible_modules = sys.modules[__name__]\n",
    "        visible_class_names, visible_classes = zip(*inspect.getmembers(visible_modules, inspect.isclass))\n",
    "        for filter_descr in self._metadata[variable]['filters']:\n",
    "            filter_cls_name = filter_descr['class']\n",
    "            filter_cls_kwargs = filter_descr['kwargs']\n",
    "            if filter_cls_name not in visible_class_names:\n",
    "                raise ValueError(f'unknown filter of type `{cfilter[\"class\"]}`')\n",
    "            filter_cls = visible_classes[visible_class_names.index(filter_cls_name)]\n",
    "            cfilter = filter_cls(**filter_cls_kwargs)\n",
    "            data = cfilter.decode(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "class DailyDatasets:\n",
    "    def __init__(self, path):\n",
    "        root = Path(path)\n",
    "        relpaths = [p for p in root.iterdir() if p.is_dir()]\n",
    "        self._datasets = sorted([DailyDataset(p) for p in relpaths], key=lambda dd: dd.year)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"[\" + ', '.join([f\"DailyDataset@{dd._path}\" for dd in self._datasets]) + \"]\"\n",
    "\n",
    "    def _iter_times(self, times):\n",
    "        for dd in self._datasets:\n",
    "            lower_bound = np.datetime64(dd.year-1970, 'Y').astype('datetime64[ns]')\n",
    "            upper_bound = np.datetime64(dd.year+1-1970, 'Y').astype('datetime64[ns]')\n",
    "            yield dd, (lower_bound <= times) & (times < upper_bound)\n",
    "\n",
    "    def get_elevation(self):\n",
    "        return self._datasets[0].get_elevation()\n",
    "\n",
    "    def get(self, variable, times=None, lons=None, lats=None, regrid_kwargs=None, interp_kwargs=None):\n",
    "        kwargs = dict(lons=lons, lats=lats, regrid_kwargs=regrid_kwargs, interp_kwargs=interp_kwargs)\n",
    "        if times is None:\n",
    "            return np.vstack([dd.get(variable, times, **kwargs) for dd in self._datasets])\n",
    "        return np.vstack([dd.get(variable, times[domain], **kwargs) for dd, domain in self._iter_times(times)])\n",
    "\n",
    "    def get_atmos(self, times=None, lons=None, lats=None, regrid_kwargs=None, interp_kwargs=None):\n",
    "        kwargs = dict(lons=lons, lats=lats, regrid_kwargs=regrid_kwargs, interp_kwargs=interp_kwargs)\n",
    "\n",
    "        atmos = defaultdict(list)\n",
    "        if times is None:\n",
    "            for dd in self._datasets:\n",
    "                for variable, values in dd.get_atmos(**({'times': times} | kwargs)).items():\n",
    "                    atmos[variable].append(values)\n",
    "        else:\n",
    "            for dd, domain in self._iter_times(times):\n",
    "                for variable, values in dd.get_atmos(**({'times': times[domain]} | kwargs)).items():\n",
    "                    atmos[variable].append(values)\n",
    "\n",
    "        return {variable: np.vstack(atmos[variable]) for variable in atmos}\n",
    "\n",
    "dd = DailyDatasets('daily')\n",
    "print(dd)\n",
    "times = np.arange('1999-01-30', '2000-12-02', step=np.timedelta64(1, 'h'), dtype='datetime64[h]')\n",
    "lats = -90. + 180. * np.random.random(10)\n",
    "lons = -180. + 360. * np.random.random(10)\n",
    "%timeit atmos = dd.get_atmos(times=times, lats=lats, lons=lons)\n",
    "for variable in atmos:\n",
    "    print(variable, atmos[variable].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a83a3",
   "metadata": {},
   "source": [
    "### Explore data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "merra2_daily = DailyDataset('daily/1999')\n",
    "merra2_daily.variables, merra2_daily.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(12, 6))\n",
    "ax = pl.subplot(111, projection=ccrs.PlateCarree())\n",
    "lat = merra2_daily.get_latitude('elevation')\n",
    "lon = merra2_daily.get_longitude('elevation')\n",
    "elev = merra2_daily.get_elevation()\n",
    "pc = ax.pcolormesh(lon, lat, elev, cmap='terrain', vmin=-1400)\n",
    "ax.coastlines(linewidth=0.5, color='w')\n",
    "pl.colorbar(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "atmos = merra2_daily.get_atmos(times=np.datetime64('2000-10-10T12:33'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pl.subplots(3, 3, figsize=(12, 6))\n",
    "variables = list(atmos.keys())\n",
    "for k_plot in range(axes.size):\n",
    "    ax = axes[k_plot // 3, k_plot % 3]\n",
    "    if k_plot < len(atmos):\n",
    "        variable = variables[k_plot]\n",
    "        ax.pcolormesh(atmos[variable], cmap='jet')\n",
    "        ax.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
    "        ax.set_title(variable, fontsize=9)\n",
    "        continue\n",
    "    ax.set_visible(False)\n",
    "logger.info('plotting...')\n",
    "# TODO: regrid no funciona bien con nans. Fijate en el albedo!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab20bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'albedo'\n",
    "target_times = pd.date_range('2000-01-01T20', '2000-10-01T00', freq='D')\n",
    "target_lons, target_lats = -3.5, 37.5\n",
    "target_lons, target_lats = np.arange(-10, 10, 0.5), np.arange(35, 55, 0.5)\n",
    "target_lons, target_lats = np.meshgrid(np.arange(-10, 10, 0.5), np.arange(35, 55, 0.5))\n",
    "\n",
    "interp_data = merra2_daily.get(variable, times=target_times, lons=target_lons, lats=target_lats)\n",
    "interp_data.shape\n",
    "\n",
    "pl.plot(target_times, regrid(target_lons[0], target_lats[:, 0], interp_data, -3.5, 37.5))\n",
    "\n",
    "source_times = merra2_daily.get_times(variable)\n",
    "source_lats = merra2_daily.get_latitude(variable)\n",
    "source_lons = merra2_daily.get_longitude(variable)\n",
    "source_data = merra2_daily.get(variable)\n",
    "regridded_data = regrid(source_lons, source_lats, source_data,\n",
    "                        np.array(target_lons, ndmin=1), np.array(target_lats, ndmin=1))\n",
    "pl.plot(source_times, regrid(target_lons[0], target_lats[:, 0], regridded_data, -3.5, 37.5), ls='', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparta import SPARTA\n",
    "import sunwhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd41de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.date_range('1999-05-02', '1999-10-30', freq='T')\n",
    "latitude = 37.5\n",
    "longitude = -3.5\n",
    "sp = sunwhere.sites(times, latitude=latitude, longitude=longitude)\n",
    "merra2_daily = DailyDataset('daily/1999')\n",
    "atmos = merra2_daily.get_atmos(times=times, lons=longitude, lats=latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=times, data=atmos)\n",
    "df = df.assign(pressure=df['pressure']/1013.25)\n",
    "df[['beta', 'alpha']].plot(figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad = pd.DataFrame(index=times, data=SPARTA(cosz=sp.cosz, ecf=sp.ecf, as_dict=True, **atmos))\n",
    "rad.plot(figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907c5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
